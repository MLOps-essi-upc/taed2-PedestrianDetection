claudiamurplanchart@MacBook-Pro-de-Claudia taed2-PedestrianDetection % pytest -k test_models.py
==================================================================================================================== test session starts =====================================================================================================================
platform darwin -- Python 3.10.6, pytest-7.4.2, pluggy-1.3.0
rootdir: /Users/claudiamurplanchart/Documents/GCED/4t_curs/TAED2/taed2-PedestrianDetection
plugins: cov-4.1.0, dvc-2.58.2, anyio-3.7.1, hydra-core-1.3.2
collected 12 items / 2 deselected / 10 selected

tests/test_models.py FFFFFFFFFF                                                                                                                                                                                                                        [100%]

========================================================================================================================== FAILURES ==========================================================================================================================
_____________________________________________________________________________________ test_model_performance[tests/img_test/lying.jpeg-tests/img_test/lying_output.pth] ______________________________________________________________________________________

image_path = 'tests/img_test/lying.jpeg', target_path = 'tests/img_test/lying_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
>       assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"
E       AssertionError: Number of predictions does not match
E       assert 2 == 1
E        +  where 2 = len(tensor([0.6364, 0.0802], grad_fn=<IndexBackward0>))
E        +  and   1 = len(tensor([0.9814]))

tests/test_models.py:58: AssertionError
_____________________________________________________________________________________ test_model_performance[tests/img_test/night.jpeg-tests/img_test/night_output.pth] ______________________________________________________________________________________

image_path = 'tests/img_test/night.jpeg', target_path = 'tests/img_test/night_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
        assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"

        # Compare common labels and score differences --> NOOOOO
        common_labels = set(predictions[0]['labels']).intersection(target['labels'])
        for label in common_labels:
            pred_indices = [i for i, l in enumerate(predictions[0]['labels']) if l == label]
            target_indices = [i for i, l in enumerate(target['labels']) if l == label]
            assert all(torch.isclose(predictions[0]['scores'][pred_i], target['scores'][target_i], rtol=bbox_tolerance, atol=bbox_tolerance)
                       for pred_i, target_i in zip(pred_indices, target_indices)), "Score differences for common labels"

        # Compare bounding boxes and masks
        for pred_box, pred_mask in zip(predictions[0]['boxes'], predictions[0]['masks']):
            for target_box, target_mask in zip(target['boxes'], target['masks']):
                assert torch.allclose(pred_box, target_box, rtol=bbox_tolerance,
                                      atol=bbox_tolerance), "Bounding boxes do not match"
>               assert torch.allclose(pred_mask, target_mask, rtol=mask_tolerance,
                                      atol=mask_tolerance), "Masks do not match"
E               AssertionError: Masks do not match
E               assert False
E                +  where False = <built-in method allclose of type object at 0x10a5ad820>(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<UnbindBackward0>), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...    [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), rtol=0.05, atol=0.05)
E                +    where <built-in method allclose of type object at 0x10a5ad820> = torch.allclose

tests/test_models.py:74: AssertionError
_________________________________________________________________________________ test_model_performance[tests/img_test/sun_glare.jpeg-tests/img_test/sun_glare_output.pth] __________________________________________________________________________________

image_path = 'tests/img_test/sun_glare.jpeg', target_path = 'tests/img_test/sun_glare_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
>       assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"
E       AssertionError: Number of predictions does not match
E       assert 5 == 11
E        +  where 5 = len(tensor([0.9981, 0.7470, 0.5005, 0.3985, 0.0565], grad_fn=<IndexBackward0>))
E        +  and   11 = len(tensor([0.9876, 0.9845, 0.9825, 0.9741, 0.9683, 0.9614, 0.9572, 0.9388, 0.8663,\n        0.8653, 0.8651]))

tests/test_models.py:58: AssertionError
_________________________________________________________________________________ test_model_performance[tests/img_test/weelchair.jpeg-tests/img_test/weelchair_output.pth] __________________________________________________________________________________

image_path = 'tests/img_test/weelchair.jpeg', target_path = 'tests/img_test/weelchair_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
>       assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"
E       AssertionError: Number of predictions does not match
E       assert 1 == 2
E        +  where 1 = len(tensor([0.9503], grad_fn=<IndexBackward0>))
E        +  and   2 = len(tensor([0.9966, 0.8722]))

tests/test_models.py:58: AssertionError
________________________________________________________________________________________ test_model_performance[tests/img_test/kid.png-tests/img_test/kid_output.pth] ________________________________________________________________________________________

image_path = 'tests/img_test/kid.png', target_path = 'tests/img_test/kid_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
        assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"

        # Compare common labels and score differences --> NOOOOO
        common_labels = set(predictions[0]['labels']).intersection(target['labels'])
        for label in common_labels:
            pred_indices = [i for i, l in enumerate(predictions[0]['labels']) if l == label]
            target_indices = [i for i, l in enumerate(target['labels']) if l == label]
            assert all(torch.isclose(predictions[0]['scores'][pred_i], target['scores'][target_i], rtol=bbox_tolerance, atol=bbox_tolerance)
                       for pred_i, target_i in zip(pred_indices, target_indices)), "Score differences for common labels"

        # Compare bounding boxes and masks
        for pred_box, pred_mask in zip(predictions[0]['boxes'], predictions[0]['masks']):
            for target_box, target_mask in zip(target['boxes'], target['masks']):
                assert torch.allclose(pred_box, target_box, rtol=bbox_tolerance,
                                      atol=bbox_tolerance), "Bounding boxes do not match"
>               assert torch.allclose(pred_mask, target_mask, rtol=mask_tolerance,
                                      atol=mask_tolerance), "Masks do not match"
E               AssertionError: Masks do not match
E               assert False
E                +  where False = <built-in method allclose of type object at 0x10a5ad820>(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<UnbindBackward0>), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...    [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), rtol=0.05, atol=0.05)
E                +    where <built-in method allclose of type object at 0x10a5ad820> = torch.allclose

tests/test_models.py:74: AssertionError
___________________________________________________________________________________ test_model_performance[tests/img_test/african.jpeg-tests/img_test/african_output.pth] ____________________________________________________________________________________

image_path = 'tests/img_test/african.jpeg', target_path = 'tests/img_test/african_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
        assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"

        # Compare common labels and score differences --> NOOOOO
        common_labels = set(predictions[0]['labels']).intersection(target['labels'])
        for label in common_labels:
            pred_indices = [i for i, l in enumerate(predictions[0]['labels']) if l == label]
            target_indices = [i for i, l in enumerate(target['labels']) if l == label]
            assert all(torch.isclose(predictions[0]['scores'][pred_i], target['scores'][target_i], rtol=bbox_tolerance, atol=bbox_tolerance)
                       for pred_i, target_i in zip(pred_indices, target_indices)), "Score differences for common labels"

        # Compare bounding boxes and masks
        for pred_box, pred_mask in zip(predictions[0]['boxes'], predictions[0]['masks']):
            for target_box, target_mask in zip(target['boxes'], target['masks']):
                assert torch.allclose(pred_box, target_box, rtol=bbox_tolerance,
                                      atol=bbox_tolerance), "Bounding boxes do not match"
>               assert torch.allclose(pred_mask, target_mask, rtol=mask_tolerance,
                                      atol=mask_tolerance), "Masks do not match"
E               AssertionError: Masks do not match
E               assert False
E                +  where False = <built-in method allclose of type object at 0x10a5ad820>(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<UnbindBackward0>), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...    [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), rtol=0.05, atol=0.05)
E                +    where <built-in method allclose of type object at 0x10a5ad820> = torch.allclose

tests/test_models.py:74: AssertionError
___________________________________________________________________________________ test_model_performance[tests/img_test/bicycle.jpeg-tests/img_test/bicycle_output.pth] ____________________________________________________________________________________

image_path = 'tests/img_test/bicycle.jpeg', target_path = 'tests/img_test/bicycle_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
        assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"

        # Compare common labels and score differences --> NOOOOO
        common_labels = set(predictions[0]['labels']).intersection(target['labels'])
        for label in common_labels:
            pred_indices = [i for i, l in enumerate(predictions[0]['labels']) if l == label]
            target_indices = [i for i, l in enumerate(target['labels']) if l == label]
            assert all(torch.isclose(predictions[0]['scores'][pred_i], target['scores'][target_i], rtol=bbox_tolerance, atol=bbox_tolerance)
                       for pred_i, target_i in zip(pred_indices, target_indices)), "Score differences for common labels"

        # Compare bounding boxes and masks
        for pred_box, pred_mask in zip(predictions[0]['boxes'], predictions[0]['masks']):
            for target_box, target_mask in zip(target['boxes'], target['masks']):
                assert torch.allclose(pred_box, target_box, rtol=bbox_tolerance,
                                      atol=bbox_tolerance), "Bounding boxes do not match"
>               assert torch.allclose(pred_mask, target_mask, rtol=mask_tolerance,
                                      atol=mask_tolerance), "Masks do not match"
E               AssertionError: Masks do not match
E               assert False
E                +  where False = <built-in method allclose of type object at 0x10a5ad820>(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<UnbindBackward0>), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., ...    [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), rtol=0.05, atol=0.05)
E                +    where <built-in method allclose of type object at 0x10a5ad820> = torch.allclose

tests/test_models.py:74: AssertionError
____________________________________________________________________________ test_model_performance[tests/img_test/bicycle_tunnel.jpeg-tests/img_test/bicycle_tunnel_output.pth] _____________________________________________________________________________

image_path = 'tests/img_test/bicycle_tunnel.jpeg', target_path = 'tests/img_test/bicycle_tunnel_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
>       assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"
E       AssertionError: Number of predictions does not match
E       assert 1 == 3
E        +  where 1 = len(tensor([0.9918], grad_fn=<IndexBackward0>))
E        +  and   3 = len(tensor([0.9977, 0.9636, 0.9507]))

tests/test_models.py:58: AssertionError
_____________________________________________________________________________ test_model_performance[tests/img_test/lot_of_people.jpeg-tests/img_test/lot_of_people_output.pth] ______________________________________________________________________________

image_path = 'tests/img_test/lot_of_people.jpeg', target_path = 'tests/img_test/lot_of_people_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
>       assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"
E       AssertionError: Number of predictions does not match
E       assert 16 == 18
E        +  where 16 = len(tensor([0.9984, 0.9982, 0.9970, 0.9951, 0.9949, 0.9940, 0.9744, 0.9677, 0.9479,\n        0.6635, 0.6008, 0.3141, 0.2364, 0.0751, 0.0697, 0.0564],\n       grad_fn=<IndexBackward0>))
E        +  and   18 = len(tensor([0.9993, 0.9987, 0.9986, 0.9985, 0.9980, 0.9980, 0.9954, 0.9920, 0.9892,\n        0.9889, 0.9682, 0.9497, 0.9437, 0.8958, 0.8798, 0.8627, 0.8613, 0.8238]))

tests/test_models.py:58: AssertionError
___________________________________________________________________________________ test_model_performance[tests/img_test/bluring.jpeg-tests/img_test/bluring_output.pth] ____________________________________________________________________________________

image_path = 'tests/img_test/bluring.jpeg', target_path = 'tests/img_test/bluring_output.pth'

    @pytest.mark.parametrize(
        "image_path, target_path",
        [('tests/img_test/lying.jpeg', 'tests/img_test/lying_output.pth'),
         ('tests/img_test/night.jpeg', 'tests/img_test/night_output.pth'),
         ('tests/img_test/sun_glare.jpeg', 'tests/img_test/sun_glare_output.pth'),
         ('tests/img_test/weelchair.jpeg', 'tests/img_test/weelchair_output.pth'),
         ('tests/img_test/kid.png', 'tests/img_test/kid_output.pth'),
         ('tests/img_test/african.jpeg', 'tests/img_test/african_output.pth'),
         ('tests/img_test/bicycle.jpeg', 'tests/img_test/bicycle_output.pth'),
         ('tests/img_test/bicycle_tunnel.jpeg', 'tests/img_test/bicycle_tunnel_output.pth'),
         ('tests/img_test/lot_of_people.jpeg', 'tests/img_test/lot_of_people_output.pth'),
         ('tests/img_test/bluring.jpeg', 'tests/img_test/bluring_output.pth')]
    )
    def test_model_performance(image_path, target_path):
        # Load the image and target
        image = Image.open(image_path).convert("RGB")
        # Convert the single image to a batch (size 1)
        image = [transforms.ToTensor()(image).to(device)]

        target = torch.load(target_path, map_location=device)

        # Get model predictions for the input image
        predictions = model(image)

        # Define a tolerance level for numerical comparisons (e.g., for bounding box coordinates)
        tolerance = 1e-5

     # Define a tolerance level for numerical comparisons
        bbox_tolerance = 1.0  # Adjust this value as needed
        mask_tolerance = 0.05  # Adjust this value as needed

     # Check if the predictions are empty or not
        if not predictions[0]['boxes'].shape[0] and not target['boxes'].shape[0]:
            # Both predictions and targets are empty; nothing to compare
            return

        # Filter out predictions with scores lower than the threshold
        score_threshold = 0.8
        above_threshold = predictions[0]['scores'] >= score_threshold
        predictions[0]['boxes'] = predictions[0]['boxes'][above_threshold]
        predictions[0]['masks'] = predictions[0]['masks'][above_threshold]

        # Check the number of predictions
>       assert len(predictions[0]['scores']) == len(
            target['scores']), "Number of predictions does not match"
E       AssertionError: Number of predictions does not match
E       assert 4 == 1
E        +  where 4 = len(tensor([0.9978, 0.9702, 0.8279, 0.0611], grad_fn=<IndexBackward0>))
E        +  and   1 = len(tensor([0.9986]))

tests/test_models.py:58: AssertionError
================================================================================================================== short test summary info ===================================================================================================================
FAILED tests/test_models.py::test_model_performance[tests/img_test/lying.jpeg-tests/img_test/lying_output.pth] - AssertionError: Number of predictions does not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/night.jpeg-tests/img_test/night_output.pth] - AssertionError: Masks do not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/sun_glare.jpeg-tests/img_test/sun_glare_output.pth] - AssertionError: Number of predictions does not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/weelchair.jpeg-tests/img_test/weelchair_output.pth] - AssertionError: Number of predictions does not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/kid.png-tests/img_test/kid_output.pth] - AssertionError: Masks do not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/african.jpeg-tests/img_test/african_output.pth] - AssertionError: Masks do not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/bicycle.jpeg-tests/img_test/bicycle_output.pth] - AssertionError: Masks do not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/bicycle_tunnel.jpeg-tests/img_test/bicycle_tunnel_output.pth] - AssertionError: Number of predictions does not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/lot_of_people.jpeg-tests/img_test/lot_of_people_output.pth] - AssertionError: Number of predictions does not match
FAILED tests/test_models.py::test_model_performance[tests/img_test/bluring.jpeg-tests/img_test/bluring_output.pth] - AssertionError: Number of predictions does not match
============================================================================================================= 10 failed, 2 deselected in 26.10s ==============================================================================================================
claudiamurplanchart@MacBook-Pro-de-Claudia taed2-PedestrianDetection %
















claudiamurplanchart@MacBook-Pro-de-Claudia taed2-PedestrianDetection % pytest -k test_negative_det.py
==================================================================================================================== test session starts =====================================================================================================================
platform darwin -- Python 3.10.6, pytest-7.4.2, pluggy-1.3.0
rootdir: /Users/claudiamurplanchart/Documents/GCED/4t_curs/TAED2/taed2-PedestrianDetection
plugins: cov-4.1.0, dvc-2.58.2, anyio-3.7.1, hydra-core-1.3.2
collected 12 items / 11 deselected / 1 selected

tests/test_negative_det.py .                                                                                                                                                                                                                           [100%]

============================================================================================================== 1 passed, 11 deselected in 4.72s ==============================================================================================================
claudiamurplanchart@MacBook-Pro-de-Claudia taed2-PedestrianDetection % pytest -k test_positive_det.py
==================================================================================================================== test session starts =====================================================================================================================
platform darwin -- Python 3.10.6, pytest-7.4.2, pluggy-1.3.0
rootdir: /Users/claudiamurplanchart/Documents/GCED/4t_curs/TAED2/taed2-PedestrianDetection
plugins: cov-4.1.0, dvc-2.58.2, anyio-3.7.1, hydra-core-1.3.2
collected 12 items / 11 deselected / 1 selected

tests/test_positive_det.py .                                                                                                                                                                                                                           [100%]

============================================================================================================== 1 passed, 11 deselected in 3.74s ==============================================================================================================
claudiamurplanchart@MacBook-Pro-de-Claudia taed2-PedestrianDetection % 
